{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab41c89-67b8-4abe-8948-776d8b484efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d72d0-67de-4cd0-92fc-57a2db7520f7",
   "metadata": {},
   "source": [
    "* Raw data directory Location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e831dd50-cbe6-45eb-8f64-16e91bda5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = os.path.join('..', 'data', 'raw')\n",
    "\n",
    "traffic_data_filename = 'Scats Data October 2006.xls'\n",
    "location_data_filename = 'Traffic_Count_Locations_with_LONG_LAT.csv'\n",
    "scats_list_filename = 'SCATSSiteListingSpreadsheet_VicRoads.xls'\n",
    "\n",
    "traffic_data_path = os.path.join(RAW_DATA_DIR, traffic_data_filename)\n",
    "location_data_path = os.path.join(RAW_DATA_DIR, location_data_filename)\n",
    "scats_list_path = os.path.join(RAW_DATA_DIR, scats_list_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216c90e-25c1-4562-a9cd-c1c7fe3dea67",
   "metadata": {},
   "source": [
    "* Load Traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3accb771-247f-4eee-97e9-43f74b80661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading traffic data from: ../data/raw/Scats Data October 2006.xls\n",
      "Traffic data loaded successfully.\n",
      "\n",
      "First 5 rows of traffic data:\n",
      "   SCATS Number                         Location CD_MELWAY  NB_LATITUDE  \\\n",
      "0           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "1           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "2           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "3           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "4           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "\n",
      "   NB_LONGITUDE  HF VicRoads Internal  VR Internal Stat  VR Internal Loc  \\\n",
      "0     145.09159                   249               182                1   \n",
      "1     145.09159                   249               182                1   \n",
      "2     145.09159                   249               182                1   \n",
      "3     145.09159                   249               182                1   \n",
      "4     145.09159                   249               182                1   \n",
      "\n",
      "   NB_TYPE_SURVEY                Date  ...  V86  V87  V88  V89  V90  V91  V92  \\\n",
      "0               1 2006-10-01 00:15:00  ...  114   97   97   66   81   50   59   \n",
      "1               1 2006-10-02 00:15:00  ...  111  102  107  114   80   60   62   \n",
      "2               1 2006-10-03 00:15:00  ...  130  132  114   86   93   90   73   \n",
      "3               1 2006-10-04 00:15:00  ...  115  113  132  101  113   90   78   \n",
      "4               1 2006-10-05 00:15:00  ...  171  120  116  113   99   91   61   \n",
      "\n",
      "   V93  V94  V95  \n",
      "0   47   29   34  \n",
      "1   48   44   26  \n",
      "2   57   29   40  \n",
      "3   66   52   44  \n",
      "4   55   49   36  \n",
      "\n",
      "[5 rows x 106 columns]\n",
      "\n",
      "Traffic data summary (info):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4192 entries, 0 to 4191\n",
      "Columns: 106 entries, SCATS Number to V95\n",
      "dtypes: datetime64[ns](1), float64(2), int64(101), object(2)\n",
      "memory usage: 3.4+ MB\n",
      "\n",
      "Missing values per column in traffic data:\n",
      "SCATS Number    0\n",
      "Location        0\n",
      "CD_MELWAY       0\n",
      "NB_LATITUDE     0\n",
      "NB_LONGITUDE    0\n",
      "               ..\n",
      "V91             0\n",
      "V92             0\n",
      "V93             0\n",
      "V94             0\n",
      "V95             0\n",
      "Length: 106, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading traffic data from: {traffic_data_path}\")\n",
    "try:\n",
    "    # This data seems to have header info on the second row (index 1),\n",
    "    # so we tell pandas to use that row for column names.\n",
    "    traffic_df = pd.read_excel(traffic_data_path, sheet_name='Data', header=1)\n",
    "    print(\"Traffic data loaded successfully.\")\n",
    "\n",
    "    # Let's look at the first few rows to see how it looks\n",
    "    print(\"\\nFirst 5 rows of traffic data:\")\n",
    "    print(traffic_df.head())\n",
    "\n",
    "    # Get a summary: column names, count of non-missing values, data types\n",
    "    print(\"\\nTraffic data summary (info):\")\n",
    "    traffic_df.info()\n",
    "\n",
    "    # Check how many missing values are in each column\n",
    "    print(\"\\nMissing values per column in traffic data:\")\n",
    "    print(traffic_df.isnull().sum())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Traffic data file not found at {traffic_data_path}\")\n",
    "    print(\"Please ensure the file exists in the data/raw directory.\")\n",
    "    traffic_df = None # Set to None if loading failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e7069-16ca-43ea-85da-47d26a8e93ba",
   "metadata": {},
   "source": [
    "* Date conversion block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5c8830-7df9-48f7-ab7e-319a26810c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'Date' column: datetime64[ns]\n",
      "\n",
      "First 5 values of 'Date' column:\n",
      "0   2006-10-01 00:15:00\n",
      "1   2006-10-02 00:15:00\n",
      "2   2006-10-03 00:15:00\n",
      "3   2006-10-04 00:15:00\n",
      "4   2006-10-05 00:15:00\n",
      "Name: Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# --- Inspect Date Column Before Conversion ---\n",
    "# Ensure traffic_df exists and has loaded successfully before running this\n",
    "if 'traffic_df' in locals() and traffic_df is not None:\n",
    "    if 'Date' in traffic_df.columns:\n",
    "        print(\"Data type of 'Date' column:\", traffic_df['Date'].dtype)\n",
    "        print(\"\\nFirst 5 values of 'Date' column:\")\n",
    "        print(traffic_df['Date'].head())\n",
    "    else:\n",
    "        print(\"'Date' column not found in DataFrame!\")\n",
    "else:\n",
    "    print(\"Variable 'traffic_df' does not exist or was not loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e12a6-476a-4957-8fed-e9e2924294e2",
   "metadata": {},
   "source": [
    "* Describe block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63107a5-e900-4644-bd3f-a360570d1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics for traffic data:\n",
      "        SCATS Number                  Location CD_MELWAY  NB_LATITUDE  \\\n",
      "count    4192.000000                      4192      4192  4192.000000   \n",
      "unique           NaN                       139        40          NaN   \n",
      "top              NaN  HIGH_ST NE of CHARLES_ST   059 J01          NaN   \n",
      "freq             NaN                        62       186          NaN   \n",
      "mean     3639.151718                       NaN       NaN   -37.542437   \n",
      "min       970.000000                       NaN       NaN   -37.867600   \n",
      "25%      3122.000000                       NaN       NaN   -37.833120   \n",
      "50%      4032.000000                       NaN       NaN   -37.822846   \n",
      "75%      4263.000000                       NaN       NaN   -37.808920   \n",
      "max      4821.000000                       NaN       NaN     0.000000   \n",
      "std       805.584115                       NaN       NaN     3.240889   \n",
      "\n",
      "        NB_LONGITUDE  HF VicRoads Internal  VR Internal Stat  VR Internal Loc  \\\n",
      "count    4192.000000           4192.000000       4192.000000      4192.000000   \n",
      "unique           NaN                   NaN               NaN              NaN   \n",
      "top              NaN                   NaN               NaN              NaN   \n",
      "freq             NaN                   NaN               NaN              NaN   \n",
      "mean      143.985065          10234.837786        819.153626         4.252147   \n",
      "min         0.000000             -1.000000        182.000000         1.000000   \n",
      "25%       145.036169           5887.000000        380.000000         3.000000   \n",
      "50%       145.058038          10074.000000        650.000000         5.000000   \n",
      "75%       145.077826          16149.000000        950.000000         7.000000   \n",
      "max       145.098850          20314.000000       2707.000000         8.000000   \n",
      "std        12.429455           5579.371363        628.218870         2.194029   \n",
      "\n",
      "        NB_TYPE_SURVEY                           Date  ...          V86  \\\n",
      "count           4192.0                           4192  ...  4192.000000   \n",
      "unique             NaN                            NaN  ...          NaN   \n",
      "top                NaN                            NaN  ...          NaN   \n",
      "freq               NaN                            NaN  ...          NaN   \n",
      "mean               1.0  2006-10-15 19:11:20.152671744  ...    76.283874   \n",
      "min                1.0            2006-10-01 00:15:00  ...     0.000000   \n",
      "25%                1.0            2006-10-08 00:15:00  ...    51.000000   \n",
      "50%                1.0            2006-10-16 00:15:00  ...    70.000000   \n",
      "75%                1.0            2006-10-23 00:15:00  ...    97.000000   \n",
      "max                1.0            2006-10-31 00:15:00  ...   334.000000   \n",
      "std                0.0                            NaN  ...    37.577091   \n",
      "\n",
      "                V87          V88          V89          V90          V91  \\\n",
      "count   4192.000000  4192.000000  4192.000000  4192.000000  4192.000000   \n",
      "unique          NaN          NaN          NaN          NaN          NaN   \n",
      "top             NaN          NaN          NaN          NaN          NaN   \n",
      "freq            NaN          NaN          NaN          NaN          NaN   \n",
      "mean      73.049380    72.192748    65.655057    61.398378    56.124761   \n",
      "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       48.000000    46.000000    42.000000    37.000000    33.000000   \n",
      "50%       67.000000    66.000000    60.000000    55.000000    50.000000   \n",
      "75%       95.000000    93.000000    85.000000    81.000000    74.000000   \n",
      "max      259.000000   275.000000   275.000000   231.000000   256.000000   \n",
      "std       36.049374    36.669158    33.885956    33.121580    32.505183   \n",
      "\n",
      "                V92          V93          V94          V95  \n",
      "count   4192.000000  4192.000000  4192.000000  4192.000000  \n",
      "unique          NaN          NaN          NaN          NaN  \n",
      "top             NaN          NaN          NaN          NaN  \n",
      "freq            NaN          NaN          NaN          NaN  \n",
      "mean      52.262643    45.383111    40.827529    35.408158  \n",
      "min        0.000000     0.000000     0.000000     0.000000  \n",
      "25%       29.000000    24.000000    20.000000    17.000000  \n",
      "50%       45.000000    39.000000    33.000000    28.000000  \n",
      "75%       68.000000    59.000000    53.000000    46.000000  \n",
      "max      324.000000   221.000000   279.000000   212.000000  \n",
      "std       33.070764    30.096586    30.012899    27.518453  \n",
      "\n",
      "[11 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Get Traffic Statistics ---\n",
    "if traffic_df is not None:\n",
    "  # Get basic statistics for numerical columns (like the V00-V95 counts)\n",
    "  print(\"\\nDescriptive statistics for traffic data:\")\n",
    "  # Using .describe(include='all') shows stats for non-numeric too if useful\n",
    "  print(traffic_df.describe(include='all'))\n",
    "else:\n",
    "  print(\"\\nSkipping descriptive statistics as traffic_df failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8960962-89d6-40f5-97ea-311f5d7b3827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with Zero Latitude or Longitude: 31\n",
      "\n",
      "SCATS Numbers corresponding to rows with Zero Coordinates:\n",
      "[4266]\n"
     ]
    }
   ],
   "source": [
    "# --- Investigate Zero Latitude/Longitude ---\n",
    "# Ensure traffic_df exists and has loaded successfully before running this\n",
    "if 'traffic_df' in locals() and traffic_df is not None:\n",
    "    zero_coord_rows = traffic_df[(traffic_df['NB_LATITUDE'] == 0) | (traffic_df['NB_LONGITUDE'] == 0)]\n",
    "    print(f\"Number of rows with Zero Latitude or Longitude: {len(zero_coord_rows)}\")\n",
    "    if not zero_coord_rows.empty:\n",
    "        print(\"\\nSCATS Numbers corresponding to rows with Zero Coordinates:\")\n",
    "        # Using unique() ensures we list each affected SCATS site only once, sorted() makes it tidier\n",
    "        print(sorted(zero_coord_rows['SCATS Number'].unique()))\n",
    "        # Optional: display some of these rows\n",
    "        # print(\"\\nExample rows with Zero Coordinates:\")\n",
    "        # print(zero_coord_rows.head())\n",
    "    else:\n",
    "        print(\"No rows found with zero coordinates.\")\n",
    "else:\n",
    "    print(\"Variable 'traffic_df' does not exist or was not loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68418f-a49b-4936-a297-6eb19499a1f8",
   "metadata": {},
   "source": [
    "* location df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6f09074-a35f-489c-819f-9d98703e86b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load CSV from: ../data/raw/Traffic_Count_Locations_with_LONG_LAT.csv\n",
      "CSV loaded successfully.\n",
      "\n",
      "--- location_df Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57947 entries, 0 to 57946\n",
      "Data columns (total 20 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   X           57947 non-null  float64\n",
      " 1   Y           57947 non-null  float64\n",
      " 2   FID         57947 non-null  int64  \n",
      " 3   OBJECTID    57947 non-null  int64  \n",
      " 4   TFM_ID      57947 non-null  int64  \n",
      " 5   TFM_DESC    57947 non-null  object \n",
      " 6   TFM_TYP_DE  57947 non-null  object \n",
      " 7   MOVEMENT_T  57947 non-null  object \n",
      " 8   SITE_DESC   57947 non-null  object \n",
      " 9   ROAD_NBR    57947 non-null  int64  \n",
      " 10  DECLARED_R  57947 non-null  object \n",
      " 11  LOCAL_ROAD  57947 non-null  object \n",
      " 12  DATA_SRC_C  57947 non-null  object \n",
      " 13  DATA_SOURC  57947 non-null  object \n",
      " 14  TIME_CATEG  57947 non-null  object \n",
      " 15  YEAR_SINCE  57947 non-null  int64  \n",
      " 16  LAST_YEAR   57947 non-null  int64  \n",
      " 17  AADT_ALLVE  57947 non-null  int64  \n",
      " 18  AADT_TRUCK  57947 non-null  int64  \n",
      " 19  PER_TRUCKS  57947 non-null  float64\n",
      "dtypes: float64(3), int64(8), object(9)\n",
      "memory usage: 8.8+ MB\n",
      "\n",
      "--- location_df Head ---\n",
      "            X          Y   FID  OBJECTID  TFM_ID  \\\n",
      "0  144.250614 -36.779313  7001      7301    7656   \n",
      "1  145.356779 -37.835309  7002      7302   29406   \n",
      "2  144.988844 -37.824629  7003      7303   22676   \n",
      "3  144.932442 -37.803783  7004      7304   27902   \n",
      "4  145.030601 -37.660502  7005      7305   10935   \n",
      "\n",
      "                                TFM_DESC    TFM_TYP_DE MOVEMENT_T  \\\n",
      "0                CALDER HWY NE OF OAK ST  INTERSECTION  All Moves   \n",
      "1  MT DANDENONG RD S BD SE OF UPALONG RD  INTERSECTION  All Moves   \n",
      "2              SWAN ST W BD E OF PUNT RD  INTERSECTION  All Moves   \n",
      "3        DYNON RD W BD E OF RADCLIFFE ST  INTERSECTION  All Moves   \n",
      "4               DALTON RD N of CHILDS RD  INTERSECTION  All Moves   \n",
      "\n",
      "                            SITE_DESC  ROAD_NBR            DECLARED_R  \\\n",
      "0                 CALDER HWY & OAK ST      2530        CALDER HIGHWAY   \n",
      "1    MT DANDENONG RD SE OF UPALONG RD      4991  MOUNT DANDENONG ROAD   \n",
      "2  PUNT RD LEFT TURN TO SWAN ST OD:12      2080        HODDLE HIGHWAY   \n",
      "3             DYNON RD & RADCLIFFE ST      5035            DYNON ROAD   \n",
      "4               CHILDS RD & DALTON RD      5605           DALTON ROAD   \n",
      "\n",
      "                     LOCAL_ROAD DATA_SRC_C      DATA_SOURC  \\\n",
      "0                   HIGH STREET      TMVMT          Manual   \n",
      "1  MOUNT DANDENONG TOURIST ROAD      APARX  Classification   \n",
      "2                     PUNT ROAD      MOTSV          Manual   \n",
      "3                    DYNON ROAD      TMVMT          Manual   \n",
      "4                   DALTON ROAD      TMVMT          Manual   \n",
      "\n",
      "              TIME_CATEG  YEAR_SINCE  LAST_YEAR  AADT_ALLVE  AADT_TRUCK  \\\n",
      "0  Greater than 10 Years          19       1997        7700         330   \n",
      "1  Greater than 10 Years          16       2000        1900           0   \n",
      "2  Greater than 10 Years          23       1993       15000         630   \n",
      "3  Greater than 10 Years          19       1997       12000        1300   \n",
      "4  Greater than 10 Years          19       1997       12000         600   \n",
      "\n",
      "   PER_TRUCKS  \n",
      "0        0.04  \n",
      "1        0.00  \n",
      "2        0.04  \n",
      "3        0.10  \n",
      "4        0.05  \n"
     ]
    }
   ],
   "source": [
    "# --- Load location_df (from Traffic_Count_Locations_with_LONG_LAT.csv) ---\n",
    "\n",
    "# First, make sure the 'location_data_path' variable from an earlier cell\n",
    "# (likely Cell 2 where you defined file paths) exists and is correct.\n",
    "# If you don't have it, make sure that cell has been run. You might need to run:\n",
    "# location_data_filename = 'Traffic_Count_Locations_with_LONG_LAT.csv'\n",
    "# location_data_path = os.path.join(RAW_DATA_DIR, location_data_filename)\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to load CSV from: {location_data_path}\")\n",
    "    location_df = pd.read_csv(location_data_path)\n",
    "    print(\"CSV loaded successfully.\")\n",
    "\n",
    "    print(\"\\n--- location_df Info ---\")\n",
    "    location_df.info()\n",
    "    print(\"\\n--- location_df Head ---\")\n",
    "    print(location_df.head())\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: 'location_data_path' variable is not defined.\")\n",
    "    print(\"Please ensure the cell defining file paths (usually Cell 2) has been run.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {location_data_path}\")\n",
    "    print(\"Please ensure the file is in the correct data/raw directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load location_df: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafd983-4f8c-417a-8d1e-e2353588ca31",
   "metadata": {},
   "source": [
    "* scats meta df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88c331a1-84fb-4f7e-b0c0-fdaa743c00f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load Excel from: ../data/raw/SCATSSiteListingSpreadsheet_VicRoads.xls\n",
      "*** formula/tFunc unknown FuncID:186\n",
      "*** formula/tFunc unknown FuncID:186\n",
      "*** formula/tFunc unknown FuncID:186\n",
      "*** formula/tFunc unknown FuncID:186\n",
      "*** formula/tFunc unknown FuncID:186\n",
      "*** formula/tFunc unknown FuncID:186\n",
      "Could not load scats_meta_df: \n"
     ]
    }
   ],
   "source": [
    "# --- Load scats_meta_df (from SCATSSiteListingSpreadsheet_VicRoads.xls) ---\n",
    "\n",
    "# First, make sure the 'scats_list_path' variable from an earlier cell\n",
    "# (likely Cell 2 where you defined file paths) exists and is correct.\n",
    "# If you don't have it, make sure that cell has been run. You might need to run:\n",
    "# scats_list_filename = 'SCATSSiteListingSpreadsheet_VicRoads.xls'\n",
    "# scats_list_path = os.path.join(RAW_DATA_DIR, scats_list_filename)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nAttempting to load Excel from: {scats_list_path}\")\n",
    "    # Load the Excel file to inspect sheets first\n",
    "    xls = pd.ExcelFile(scats_list_path)\n",
    "    print(\"Excel file opened successfully. Sheet names are:\")\n",
    "    print(xls.sheet_names)\n",
    "\n",
    "    # *** ACTION REQUIRED: Check the sheet names printed above ***\n",
    "    # Choose the sheet name that seems most relevant (e.g., contains site list,\n",
    "    # locations, SCATS numbers). If unsure, the first sheet is often a good start.\n",
    "    # Replace xls.sheet_names[0] below if needed.\n",
    "    sheet_to_load = xls.sheet_names[0] # <--- CHANGE index [0] or name 'Sheet1' etc. if needed\n",
    "\n",
    "    print(f\"\\nLoading sheet: '{sheet_to_load}'...\")\n",
    "    # Now load the chosen sheet into the DataFrame\n",
    "    scats_meta_df = pd.read_excel(xls, sheet_name=sheet_to_load)\n",
    "    print(\"Sheet loaded successfully.\")\n",
    "\n",
    "    print(\"\\n--- scats_meta_df Info ---\")\n",
    "    scats_meta_df.info()\n",
    "    print(\"\\n--- scats_meta_df Head ---\")\n",
    "    print(scats_meta_df.head())\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: 'scats_list_path' variable is not defined.\")\n",
    "    print(\"Please ensure the cell defining file paths (usually Cell 2) has been run.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {scats_list_path}\")\n",
    "    print(\"Please ensure the file is in the correct data/raw directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load scats_meta_df: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2660203f-8cff-456c-aadc-007620bf3cb9",
   "metadata": {},
   "source": [
    "* tfm id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c3f4609-0d1b-4e3d-9757-c1c6175b4121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TFM_ID Statistics ---\n",
      "count    57947.000000\n",
      "mean     30181.799023\n",
      "std      17526.353152\n",
      "min          2.000000\n",
      "25%      14955.500000\n",
      "50%      30019.000000\n",
      "75%      45378.500000\n",
      "max      61111.000000\n",
      "Name: TFM_ID, dtype: float64\n",
      "\n",
      "--- Example Unique TFM_ID values (first 20) ---\n",
      "[ 7656 29406 22676 27902 10935 27853 10936 26639 11632 22114 13918 21470\n",
      " 28351 28732 11438  9899 27958 27883 28274  9684]\n",
      "\n",
      "--- Are there many unique TFM_IDs? ---\n",
      "Total unique TFM_IDs: 57947\n"
     ]
    }
   ],
   "source": [
    "# --- Check TFM_ID values ---\n",
    "# Compare these stats/values to the SCATS Numbers in traffic_df (min 970, max 4821 from earlier describe)\n",
    "if 'location_df' in locals() and location_df is not None:\n",
    "     print(\"\\n--- TFM_ID Statistics ---\")\n",
    "     # Check the min/max range of TFM_ID\n",
    "     print(location_df['TFM_ID'].describe())\n",
    "     print(\"\\n--- Example Unique TFM_ID values (first 20) ---\")\n",
    "     # Check if these first few unique values look like the SCATS numbers\n",
    "     print(location_df['TFM_ID'].unique()[:20])\n",
    "     print(\"\\n--- Are there many unique TFM_IDs? ---\")\n",
    "     # How many unique IDs are there in this file?\n",
    "     print(f\"Total unique TFM_IDs: {location_df['TFM_ID'].nunique()}\")\n",
    "else:\n",
    "     print(\"location_df not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd4680-94c7-4ee5-af09-9d541af81a87",
   "metadata": {},
   "source": [
    "* tfm desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f25a9b1-052e-4534-acee-8a6296608e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example TFM_DESC values (first 20) ---\n",
      "0                          CALDER HWY NE OF OAK ST\n",
      "1            MT DANDENONG RD S BD SE OF UPALONG RD\n",
      "2                        SWAN ST W BD E OF PUNT RD\n",
      "3                  DYNON RD W BD E OF RADCLIFFE ST\n",
      "4                         DALTON RD N of CHILDS RD\n",
      "5               GRIMSHAW ST E of GREENSBOROUGH HWY\n",
      "6               BULLEEN RD N BD N of MANNINGHAM RD\n",
      "7                 FRANKSTON DANDENON NE of SKYE RD\n",
      "8                   PRINCESS ST N OF HUTCHINSON DR\n",
      "9            PASCOE VALE RD N BD NE OF DIMBOOLA RD\n",
      "10                  MOOROODUC HWY N OF HASTINGS RD\n",
      "11            ACHERON WAY N BD S OF DONNA BUANG RD\n",
      "12           TODD RD SEBD SE OF WEST GATE FWY CONN\n",
      "13              MAROONDAH HWY E BD E of STATION ST\n",
      "14    LOWER HEIDELBERG RD 10M AFTER RAVENSWOOD AVE\n",
      "15            STUDLEY PARK RD 50M AFTER HODGSON ST\n",
      "16         MAROONDAH HWY SWBD SW OF BONNIE VIEW RD\n",
      "17             CANTERBURY RD SEBD NW OF FITZROY ST\n",
      "18                    NEPEAN HWY NWBD SE OF MAY ST\n",
      "19                    NICHOLSON ST N OF HARDING ST\n",
      "Name: TFM_DESC, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- Check TFM_DESC values ---\n",
    "# Look for SCATS numbers embedded in these descriptions (e.g., \"Site 4266...\")\n",
    "if 'location_df' in locals() and location_df is not None:\n",
    "    print(\"\\n--- Example TFM_DESC values (first 20) ---\")\n",
    "    # Print more values to see if SCATS numbers appear, show full text\n",
    "    with pd.option_context('display.max_colwidth', None): # Ensure full description is shown\n",
    "        print(location_df['TFM_DESC'].head(20))\n",
    "else:\n",
    "     print(\"location_df not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6699053-a4e5-4ede-b917-f489f062220d",
   "metadata": {},
   "source": [
    "* step 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c841eafc-317a-47c3-bd19-f1db58c206ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reload cleaned CSV from: ../data/raw/SCATS_Site_List_Cleaned.csv specifying header=1 (row 2)\n",
      "Cleaned SCATS site list reloaded successfully with correct header.\n",
      "\n",
      "--- (Reloaded) scats_locations_df Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4192 entries, 0 to 4191\n",
      "Data columns (total 109 columns):\n",
      " #    Column                Non-Null Count  Dtype  \n",
      "---   ------                --------------  -----  \n",
      " 0    SCATS Number          4192 non-null   int64  \n",
      " 1    Location              4192 non-null   object \n",
      " 2    CD_MELWAY             4192 non-null   object \n",
      " 3    NB_LATITUDE           4192 non-null   float64\n",
      " 4    NB_LONGITUDE          4192 non-null   float64\n",
      " 5    HF VicRoads Internal  4192 non-null   int64  \n",
      " 6    VR Internal Stat      4192 non-null   int64  \n",
      " 7    VR Internal Loc       4192 non-null   int64  \n",
      " 8    NB_TYPE_SURVEY        4192 non-null   int64  \n",
      " 9    Date                  4192 non-null   object \n",
      " 10   V00                   4192 non-null   int64  \n",
      " 11   V01                   4192 non-null   int64  \n",
      " 12   V02                   4192 non-null   int64  \n",
      " 13   V03                   4192 non-null   int64  \n",
      " 14   V04                   4192 non-null   int64  \n",
      " 15   V05                   4192 non-null   int64  \n",
      " 16   V06                   4192 non-null   int64  \n",
      " 17   V07                   4192 non-null   int64  \n",
      " 18   V08                   4192 non-null   int64  \n",
      " 19   V09                   4192 non-null   int64  \n",
      " 20   V10                   4192 non-null   int64  \n",
      " 21   V11                   4192 non-null   int64  \n",
      " 22   V12                   4192 non-null   int64  \n",
      " 23   V13                   4192 non-null   int64  \n",
      " 24   V14                   4192 non-null   int64  \n",
      " 25   V15                   4192 non-null   int64  \n",
      " 26   V16                   4192 non-null   int64  \n",
      " 27   V17                   4192 non-null   int64  \n",
      " 28   V18                   4192 non-null   int64  \n",
      " 29   V19                   4192 non-null   int64  \n",
      " 30   V20                   4192 non-null   int64  \n",
      " 31   V21                   4192 non-null   int64  \n",
      " 32   V22                   4192 non-null   int64  \n",
      " 33   V23                   4192 non-null   int64  \n",
      " 34   V24                   4192 non-null   int64  \n",
      " 35   V25                   4192 non-null   int64  \n",
      " 36   V26                   4192 non-null   int64  \n",
      " 37   V27                   4192 non-null   int64  \n",
      " 38   V28                   4192 non-null   int64  \n",
      " 39   V29                   4192 non-null   int64  \n",
      " 40   V30                   4192 non-null   int64  \n",
      " 41   V31                   4192 non-null   int64  \n",
      " 42   V32                   4192 non-null   int64  \n",
      " 43   V33                   4192 non-null   int64  \n",
      " 44   V34                   4192 non-null   int64  \n",
      " 45   V35                   4192 non-null   int64  \n",
      " 46   V36                   4192 non-null   int64  \n",
      " 47   V37                   4192 non-null   int64  \n",
      " 48   V38                   4192 non-null   int64  \n",
      " 49   V39                   4192 non-null   int64  \n",
      " 50   V40                   4192 non-null   int64  \n",
      " 51   V41                   4192 non-null   int64  \n",
      " 52   V42                   4192 non-null   int64  \n",
      " 53   V43                   4192 non-null   int64  \n",
      " 54   V44                   4192 non-null   int64  \n",
      " 55   V45                   4192 non-null   int64  \n",
      " 56   V46                   4192 non-null   int64  \n",
      " 57   V47                   4192 non-null   int64  \n",
      " 58   V48                   4192 non-null   int64  \n",
      " 59   V49                   4192 non-null   int64  \n",
      " 60   V50                   4192 non-null   int64  \n",
      " 61   V51                   4192 non-null   int64  \n",
      " 62   V52                   4192 non-null   int64  \n",
      " 63   V53                   4192 non-null   int64  \n",
      " 64   V54                   4192 non-null   int64  \n",
      " 65   V55                   4192 non-null   int64  \n",
      " 66   V56                   4192 non-null   int64  \n",
      " 67   V57                   4192 non-null   int64  \n",
      " 68   V58                   4192 non-null   int64  \n",
      " 69   V59                   4192 non-null   int64  \n",
      " 70   V60                   4192 non-null   int64  \n",
      " 71   V61                   4192 non-null   int64  \n",
      " 72   V62                   4192 non-null   int64  \n",
      " 73   V63                   4192 non-null   int64  \n",
      " 74   V64                   4192 non-null   int64  \n",
      " 75   V65                   4192 non-null   int64  \n",
      " 76   V66                   4192 non-null   int64  \n",
      " 77   V67                   4192 non-null   int64  \n",
      " 78   V68                   4192 non-null   int64  \n",
      " 79   V69                   4192 non-null   int64  \n",
      " 80   V70                   4192 non-null   int64  \n",
      " 81   V71                   4192 non-null   int64  \n",
      " 82   V72                   4192 non-null   int64  \n",
      " 83   V73                   4192 non-null   int64  \n",
      " 84   V74                   4192 non-null   int64  \n",
      " 85   V75                   4192 non-null   int64  \n",
      " 86   V76                   4192 non-null   int64  \n",
      " 87   V77                   4192 non-null   int64  \n",
      " 88   V78                   4192 non-null   int64  \n",
      " 89   V79                   4192 non-null   int64  \n",
      " 90   V80                   4192 non-null   int64  \n",
      " 91   V81                   4192 non-null   int64  \n",
      " 92   V82                   4192 non-null   int64  \n",
      " 93   V83                   4192 non-null   int64  \n",
      " 94   V84                   4192 non-null   int64  \n",
      " 95   V85                   4192 non-null   int64  \n",
      " 96   V86                   4192 non-null   int64  \n",
      " 97   V87                   4192 non-null   int64  \n",
      " 98   V88                   4192 non-null   int64  \n",
      " 99   V89                   4192 non-null   int64  \n",
      " 100  V90                   4192 non-null   int64  \n",
      " 101  V91                   4192 non-null   int64  \n",
      " 102  V92                   4192 non-null   int64  \n",
      " 103  V93                   4192 non-null   int64  \n",
      " 104  V94                   4192 non-null   int64  \n",
      " 105  V95                   4192 non-null   int64  \n",
      " 106  Unnamed: 106          0 non-null      float64\n",
      " 107  Unnamed: 107          0 non-null      float64\n",
      " 108  Unnamed: 108          0 non-null      float64\n",
      "dtypes: float64(5), int64(101), object(3)\n",
      "memory usage: 3.5+ MB\n",
      "\n",
      "--- (Reloaded) scats_locations_df Head ---\n",
      "   SCATS Number                         Location CD_MELWAY  NB_LATITUDE  \\\n",
      "0           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "1           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "2           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "3           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "4           970  WARRIGAL_RD N of HIGH STREET_RD   060 G10    -37.86703   \n",
      "\n",
      "   NB_LONGITUDE  HF VicRoads Internal  VR Internal Stat  VR Internal Loc  \\\n",
      "0     145.09159                   249               182                1   \n",
      "1     145.09159                   249               182                1   \n",
      "2     145.09159                   249               182                1   \n",
      "3     145.09159                   249               182                1   \n",
      "4     145.09159                   249               182                1   \n",
      "\n",
      "   NB_TYPE_SURVEY       Date  V00  V01  V02  V03  V04  V05  V06  V07  V08  \\\n",
      "0               1  1/10/2006   86   83   52   58   59   44   31   37   30   \n",
      "1               1  2/10/2006   32   28   17   11    7   11    6   15   11   \n",
      "2               1  3/10/2006   26   32   21   14   10   12   13   10    9   \n",
      "3               1  4/10/2006   32   22   28   13   16    8   14   10    8   \n",
      "4               1  5/10/2006   40   39   21   11   16    9   15   15    9   \n",
      "\n",
      "   V09  V10  V11  V12  V13  V14  V15  V16  V17  V18  V19  V20  V21  V22  V23  \\\n",
      "0   24   16   24   25   25   15    6   21   17   15   15   16   21   27   21   \n",
      "1   12    6    9    1    4   11    9   11   16   22   14   28   26   57   57   \n",
      "2    7    8    5    5    6   11   12    8   11   13   10   23   37   64   95   \n",
      "3    8    7    6    8    2    7   10    8   14   16   12   24   37   62   84   \n",
      "4    6    9    4    4    1   11    9   17   13   16   15   23   35   58   79   \n",
      "\n",
      "   V24  V25  V26  V27  V28  V29  V30  V31  V32  V33  V34  V35  V36  V37  V38  \\\n",
      "0   25   32   61   48   56   66   77   79   67   93  103  130  154  149  210   \n",
      "1   70  136  221  196  239  366  355  400  401  400  395  367  315  308  302   \n",
      "2   90  183  219  251  302  307  410  351  411  408  405  372  330  366  360   \n",
      "3   82  166  230  235  256  336  316  392  374  392  417  380  376  328  324   \n",
      "4   81  139  238  235  235  328  344  315  359  434  375  365  356  362  328   \n",
      "\n",
      "   V39  V40  V41  V42  V43  V44  V45  V46  V47  V48  V49  V50  V51  V52  V53  \\\n",
      "0  229  250  246  266  254  300  275  322  292  315  314  308  280  357  298   \n",
      "1  306  245  286  279  275  227  234  236  239  320  254  242  261  251  290   \n",
      "2  317  312  289  293  299  293  273  264  264  290  260  286  272  274  270   \n",
      "3  338  282  272  290  340  301  288  258  272  319  315  251  281  294  301   \n",
      "4  330  327  276  311  281  303  324  285  301  331  296  302  299  290  281   \n",
      "\n",
      "   V54  V55  V56  V57  V58  V59  V60  V61  V62  V63  V64  V65  V66  V67  V68  \\\n",
      "0  281  289  345  297  233  227  273  225  265  257  233  244  259  264  265   \n",
      "1  254  256  283  226  276  271  281  285  306  301  327  340  294  326  277   \n",
      "2  259  313  281  347  280  266  347  277  328  318  357  347  381  288  358   \n",
      "3  300  288  282  260  247  252  320  300  264  329  359  345  270  338  308   \n",
      "4  308  294  303  296  265  306  312  322  343  317  397  385  388  324  348   \n",
      "\n",
      "   V69  V70  V71  V72  V73  V74  V75  V76  V77  V78  V79  V80  V81  V82  V83  \\\n",
      "0  253  243  210  216  202  177  169  167  136  131  128  118  121   87  113   \n",
      "1  382  320  377  259  298  218  190  186  172  161  158  134  141  119  142   \n",
      "2  338  365  369  296  335  269  232  211  198  176  163  155  140  135  129   \n",
      "3  340  377  335  352  290  258  281  237  203  167  165  133  163  135  141   \n",
      "4  361  372  396  315  334  309  271  251  262  228  196  176  152  146  167   \n",
      "\n",
      "   V84  V85  V86  V87  V88  V89  V90  V91  V92  V93  V94  V95  Unnamed: 106  \\\n",
      "0  142  112  114   97   97   66   81   50   59   47   29   34           NaN   \n",
      "1  103  108  111  102  107  114   80   60   62   48   44   26           NaN   \n",
      "2  139  146  130  132  114   86   93   90   73   57   29   40           NaN   \n",
      "3  137  158  115  113  132  101  113   90   78   66   52   44           NaN   \n",
      "4  122  150  171  120  116  113   99   91   61   55   49   36           NaN   \n",
      "\n",
      "   Unnamed: 107  Unnamed: 108  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n"
     ]
    }
   ],
   "source": [
    "# --- Reload the cleaned SCATS Site List - Attempt 4 (Specify Header Row) ---\n",
    "cleaned_scats_filename = 'SCATS_Site_List_Cleaned.csv'\n",
    "scats_locations_df = None # Reset variable in case previous loads failed partially\n",
    "\n",
    "# Make sure RAW_DATA_DIR, os, pd are defined from earlier cells\n",
    "try:\n",
    "    cleaned_scats_path = os.path.join(RAW_DATA_DIR, cleaned_scats_filename)\n",
    "    print(f\"Attempting to reload cleaned CSV from: {cleaned_scats_path} specifying header=1 (row 2)\")\n",
    "\n",
    "    # Load the CSV file, telling pandas the header is on the SECOND row (index 1)\n",
    "    # This will automatically skip the first row (index 0)\n",
    "    scats_locations_df = pd.read_csv(cleaned_scats_path, header=1)\n",
    "\n",
    "    print(\"Cleaned SCATS site list reloaded successfully with correct header.\")\n",
    "\n",
    "    # Inspect the reloaded data\n",
    "    print(\"\\n--- (Reloaded) scats_locations_df Info ---\")\n",
    "    # Use verbose=True to see all columns if there are many\n",
    "    scats_locations_df.info(verbose=True, show_counts=True)\n",
    "    print(\"\\n--- (Reloaded) scats_locations_df Head ---\")\n",
    "    # Display more columns to see if it looks right\n",
    "    with pd.option_context('display.max_rows', 5, 'display.max_columns', None):\n",
    "         print(scats_locations_df.head())\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: 'RAW_DATA_DIR', 'os', or 'pd' variable is not defined.\")\n",
    "    print(\"Please ensure the cells defining file paths and importing pandas/os have been run.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Cleaned file not found at {cleaned_scats_path}\")\n",
    "    print(f\"Please ensure '{cleaned_scats_filename}' was saved correctly in the 'data/raw' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not reload {cleaned_scats_filename}: {e}\")\n",
    "    scats_locations_df = None # Mark as failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2bb60-c94a-4953-b96d-4db5eebcc200",
   "metadata": {},
   "source": [
    "* site coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09e0c971-1d84-4ffb-95fb-59c7de265b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique coordinates using 'SCATS Number', 'NB_LATITUDE', 'NB_LONGITUDE'\n",
      "\n",
      "Extracted 140 unique site locations.\n",
      "\n",
      "--- Unique site_coords Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   SCATS Number  140 non-null    int64  \n",
      " 1   NB_LATITUDE   140 non-null    float64\n",
      " 2   NB_LONGITUDE  140 non-null    float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 3.4 KB\n",
      "\n",
      "--- Unique site_coords Head ---\n",
      "   SCATS Number  NB_LATITUDE  NB_LONGITUDE\n",
      "0           970   -37.867030    145.091590\n",
      "1           970   -37.867350    145.091950\n",
      "2           970   -37.867600    145.091460\n",
      "3           970   -37.867230    145.091030\n",
      "4          2000   -37.851683    145.094346\n",
      "\n",
      "Coordinates for site 4266 in the cleaned data:\n",
      "     SCATS Number  NB_LATITUDE  NB_LONGITUDE\n",
      "111          4266      0.00000       0.00000\n",
      "112          4266    -37.82529     145.04387\n",
      "113          4266    -37.82542     145.04346\n",
      "114          4266    -37.82518     145.04301\n",
      "WARNING: Site 4266 still has zero coordinates (0.0, 0.0) in this cleaned data!\n"
     ]
    }
   ],
   "source": [
    "# --- Extract Unique Site Coordinates ---\n",
    "site_coords = None # Reset variable in case of errors\n",
    "\n",
    "# Ensure scats_locations_df exists and has loaded successfully before running this\n",
    "if 'scats_locations_df' in locals() and scats_locations_df is not None:\n",
    "    try:\n",
    "        # Define the exact column names we need based on the previous .info() output\n",
    "        # These look correct based on your output, but double-check if needed\n",
    "        id_col = 'SCATS Number'\n",
    "        lat_col = 'NB_LATITUDE'\n",
    "        lon_col = 'NB_LONGITUDE'\n",
    "\n",
    "        print(f\"Extracting unique coordinates using '{id_col}', '{lat_col}', '{lon_col}'\")\n",
    "        \n",
    "        # Select only the necessary columns from the DataFrame\n",
    "        site_coords_full = scats_locations_df[[id_col, lat_col, lon_col]]\n",
    "\n",
    "        # Remove duplicate rows - keeps the first unique combination found for each site\n",
    "        site_coords = site_coords_full.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\nExtracted {len(site_coords)} unique site locations.\")\n",
    "\n",
    "        # Display info and head of the new unique coordinates DataFrame\n",
    "        print(\"\\n--- Unique site_coords Info ---\")\n",
    "        site_coords.info()\n",
    "        print(\"\\n--- Unique site_coords Head ---\")\n",
    "        print(site_coords.head())\n",
    "\n",
    "        # Specifically check site 4266 (the one with 0,0 coords in the other data)\n",
    "        print(\"\\nCoordinates for site 4266 in the cleaned data:\")\n",
    "        site_4266_coords = site_coords[site_coords[id_col] == 4266]\n",
    "\n",
    "        if not site_4266_coords.empty:\n",
    "            print(site_4266_coords)\n",
    "            # Check if the coordinates found for 4266 are non-zero\n",
    "            lat_val = site_4266_coords.iloc[0][lat_col]\n",
    "            lon_val = site_4266_coords.iloc[0][lon_col]\n",
    "            if lat_val == 0 or lon_val == 0:\n",
    "                print(f\"WARNING: Site 4266 still has zero coordinates ({lat_val}, {lon_val}) in this cleaned data!\")\n",
    "            else:\n",
    "                print(f\"Site 4266 appears to have valid coordinates ({lat_val}, {lon_val}) in this cleaned data.\")\n",
    "        else:\n",
    "            print(\"Site 4266 not found in the unique coordinates list.\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        # This error happens if one of the column names ('SCATS Number', 'NB_LATITUDE', 'NB_LONGITUDE') is wrong\n",
    "        print(f\"ERROR: A required column name is missing or misspelled: {e}\")\n",
    "        print(\"Please double-check the column names in the .info() output from the previous cell and adjust the id_col, lat_col, lon_col variables in this cell if necessary.\")\n",
    "        site_coords = None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "        site_coords = None\n",
    "else:\n",
    "    print(\"Variable 'scats_locations_df' does not exist or was not loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8468384-648a-446d-b723-c8542f990d28",
   "metadata": {},
   "source": [
    "* Step 15: Select a Single, VALID Coordinate per Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19dbbf64-4b8d-4e91-94ea-81087eac5c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 140 unique site/coordinate combinations found previously.\n",
      "Found 139 combinations with non-zero coordinates.\n",
      "Final unique sites map created with 40 sites.\n",
      "\n",
      "--- Final site_coords_unique Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   SCATS Number  40 non-null     int64  \n",
      " 1   NB_LATITUDE   40 non-null     float64\n",
      " 2   NB_LONGITUDE  40 non-null     float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 1.1 KB\n",
      "\n",
      "--- Final site_coords_unique Head ---\n",
      "   SCATS Number  NB_LATITUDE  NB_LONGITUDE\n",
      "0           970   -37.867030    145.091590\n",
      "1          2000   -37.851683    145.094346\n",
      "2          2200   -37.816310    145.098120\n",
      "3          2820   -37.794770    145.030770\n",
      "4          2825   -37.786610    145.062020\n",
      "\n",
      "Final Coordinates for site 4266:\n",
      "    SCATS Number  NB_LATITUDE  NB_LONGITUDE\n",
      "31          4266    -37.82529     145.04387\n",
      "\n",
      "Final Coordinates for site 970:\n",
      "   SCATS Number  NB_LATITUDE  NB_LONGITUDE\n",
      "0           970    -37.86703     145.09159\n"
     ]
    }
   ],
   "source": [
    "# --- Select a Single, Valid Coordinate per Site ---\n",
    "site_coords_unique = None # Reset variable\n",
    "\n",
    "if 'site_coords' in locals() and site_coords is not None:\n",
    "    try:\n",
    "        # Define the exact column names based on the previous output\n",
    "        id_col = 'SCATS Number'\n",
    "        lat_col = 'NB_LATITUDE'\n",
    "        lon_col = 'NB_LONGITUDE'\n",
    "\n",
    "        print(f\"Starting with {len(site_coords)} unique site/coordinate combinations found previously.\")\n",
    "\n",
    "        # 1. Filter out invalid coordinates (where lat or lon is exactly 0)\n",
    "        # Create a boolean mask to identify rows with non-zero lat AND non-zero lon\n",
    "        valid_mask = (site_coords[lat_col] != 0) & (site_coords[lon_col] != 0)\n",
    "        # Apply the mask to keep only rows with valid coordinates\n",
    "        valid_coords = site_coords[valid_mask].copy()\n",
    "        print(f\"Found {len(valid_coords)} combinations with non-zero coordinates.\")\n",
    "\n",
    "        # 2. From these valid coordinates, keep only the *first* entry found for each SCATS Number\n",
    "        # This ensures we have only one row per SCATS site ID.\n",
    "        site_coords_unique = valid_coords.drop_duplicates(subset=[id_col], keep='first').reset_index(drop=True)\n",
    "        print(f\"Final unique sites map created with {len(site_coords_unique)} sites.\")\n",
    "\n",
    "        # Display info and head of the FINAL unique coordinates DataFrame\n",
    "        print(\"\\n--- Final site_coords_unique Info ---\")\n",
    "        site_coords_unique.info()\n",
    "        print(\"\\n--- Final site_coords_unique Head ---\")\n",
    "        print(site_coords_unique.head()) # Note site 970 should only appear once now\n",
    "\n",
    "        # --- Verification Checks ---\n",
    "        # Check site 4266 again (should now only have the first valid coordinate pair found)\n",
    "        print(\"\\nFinal Coordinates for site 4266:\")\n",
    "        final_4266_coords = site_coords_unique[site_coords_unique[id_col] == 4266]\n",
    "        if not final_4266_coords.empty:\n",
    "            print(final_4266_coords)\n",
    "        else:\n",
    "            # This would happen if site 4266 ONLY had (0,0) coordinates originally\n",
    "            print(\"Site 4266 had only invalid (0,0) coordinates and was removed, or was not present in the valid list.\")\n",
    "\n",
    "        # Check site 970 again (should only have one entry now)\n",
    "        print(\"\\nFinal Coordinates for site 970:\")\n",
    "        print(site_coords_unique[site_coords_unique[id_col] == 970])\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: A required column name ({id_col}, {lat_col}, or {lon_col}) might be slightly different: {e}\")\n",
    "        print(\"Please double-check the column names and adjust the code if necessary.\")\n",
    "        site_coords_unique = None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during final selection: {e}\")\n",
    "        site_coords_unique = None\n",
    "else:\n",
    "    print(\"Variable 'site_coords' (from previous step) does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99e010-a18c-46af-b54f-4a193187515a",
   "metadata": {},
   "source": [
    "- Step 16: Merge Coordinates with Traffic Data and 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8578491-ac8f-43e4-ad87-a4fbc15bd73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Inputs Before Merge ---\n",
      "traffic_df shape: (4192, 106)\n",
      "traffic_df unique SCATS: 40\n",
      "site_coords_unique shape: (40, 3)\n",
      "site_coords_unique unique SCATS: 40\n",
      "Does 'NB_LATITUDE' exist in site_coords_unique? True\n",
      "Does 'NB_LONGITUDE' exist in site_coords_unique? True\n",
      "------------------------------\n",
      "--- Data Types Before Merge ---\n",
      "traffic_df['SCATS Number'] dtype: int64\n",
      "site_coords_unique['SCATS Number'] dtype: int64\n",
      "\n",
      "Re-running merge using 'SCATS Number'...\n",
      "Merge completed.\n",
      "Clean coordinate columns identified as: 'NB_LATITUDE_clean', 'NB_LONGITUDE_clean'\n",
      "\n",
      "--- Checking State Immediately After Merge ---\n",
      "NaNs detected in 'NB_LATITUDE_clean' right after merge: 0 (Expected 0)\n",
      "NaNs detected in 'NB_LONGITUDE_clean' right after merge: 0 (Expected 0)\n",
      "Zeroes detected in 'NB_LATITUDE_clean' right after merge: 0\n",
      "Zeroes detected in 'NB_LONGITUDE_clean' right after merge: 0\n",
      "\n",
      "Attempting filtering using notna() AND != 0.0 ...\n",
      "Filtering complete.\n",
      "\n",
      "--- After Filtering ---\n",
      "Rows after: 4192\n",
      "NaNs in 'NB_LATITUDE_clean' after: 0 (Expected 0)\n",
      "NaNs in 'NB_LONGITUDE_clean' after: 0 (Expected 0)\n",
      "Zeroes in 'NB_LATITUDE_clean' after: 0 (Expected 0)\n",
      "Zeroes in 'NB_LONGITUDE_clean' after: 0 (Expected 0)\n",
      "Unique SCATS sites remaining: 40\n",
      "\n",
      "Filtering did NOT produce the expected result.\n",
      "  Expected: 1240 rows, 40 sites.\n",
      "  Actual:   4192 rows, 40 sites.\n",
      "  Skipping rename and final head display. Please investigate.\n",
      "\n",
      "--- final_df Head (current state, showing key cols) ---\n",
      "   SCATS Number                Date                         Location  \\\n",
      "0           970 2006-10-01 00:15:00  WARRIGAL_RD N of HIGH STREET_RD   \n",
      "1           970 2006-10-02 00:15:00  WARRIGAL_RD N of HIGH STREET_RD   \n",
      "2           970 2006-10-03 00:15:00  WARRIGAL_RD N of HIGH STREET_RD   \n",
      "3           970 2006-10-04 00:15:00  WARRIGAL_RD N of HIGH STREET_RD   \n",
      "4           970 2006-10-05 00:15:00  WARRIGAL_RD N of HIGH STREET_RD   \n",
      "\n",
      "   NB_LATITUDE_clean  NB_LONGITUDE_clean  VR Internal Stat  VR Internal Loc  \\\n",
      "0          -37.86703           145.09159               182                1   \n",
      "1          -37.86703           145.09159               182                1   \n",
      "2          -37.86703           145.09159               182                1   \n",
      "3          -37.86703           145.09159               182                1   \n",
      "4          -37.86703           145.09159               182                1   \n",
      "\n",
      "   V00  V01  V02  \n",
      "0   86   83   52  \n",
      "1   32   28   17  \n",
      "2   26   32   21  \n",
      "3   32   22   28  \n",
      "4   40   39   21  \n"
     ]
    }
   ],
   "source": [
    "# --- Step 16/17 Revised: Re-run Merge and Filter with 0.0 check ---\n",
    "\n",
    "traffic_df_linked = None # Reset variables\n",
    "final_df = None\n",
    "\n",
    "# Check if input DataFrames exist\n",
    "if ('traffic_df' in locals() and traffic_df is not None and\n",
    "    'site_coords_unique' in locals() and site_coords_unique is not None):\n",
    "\n",
    "    # +++ DIAGNOSTICS (Keep these) +++\n",
    "    print(\"--- Verifying Inputs Before Merge ---\")\n",
    "    print(f\"traffic_df shape: {traffic_df.shape}\")\n",
    "    print(f\"traffic_df unique SCATS: {traffic_df['SCATS Number'].nunique()}\")\n",
    "    print(f\"site_coords_unique shape: {site_coords_unique.shape}\")\n",
    "    print(f\"site_coords_unique unique SCATS: {site_coords_unique['SCATS Number'].nunique()}\")\n",
    "    lat_col_in_coords = 'NB_LATITUDE'\n",
    "    lon_col_in_coords = 'NB_LONGITUDE'\n",
    "    print(f\"Does '{lat_col_in_coords}' exist in site_coords_unique? {lat_col_in_coords in site_coords_unique.columns}\")\n",
    "    print(f\"Does '{lon_col_in_coords}' exist in site_coords_unique? {lon_col_in_coords in site_coords_unique.columns}\")\n",
    "    print(\"-\" * 30) # Separator\n",
    "    # +++ END DIAGNOSTICS +++\n",
    "\n",
    "    try:\n",
    "        # --- Step 16: Merge ---\n",
    "        id_col = 'SCATS Number'\n",
    "        lat_col = 'NB_LATITUDE' # Column name from site_coords_unique\n",
    "        lon_col = 'NB_LONGITUDE'# Column name from site_coords_unique\n",
    "\n",
    "        # Check data types BEFORE merge\n",
    "        print(\"--- Data Types Before Merge ---\")\n",
    "        print(f\"traffic_df['{id_col}'] dtype: {traffic_df[id_col].dtype}\")\n",
    "        print(f\"site_coords_unique['{id_col}'] dtype: {site_coords_unique[id_col].dtype}\")\n",
    "\n",
    "        coords_to_merge = site_coords_unique[[id_col, lat_col, lon_col]]\n",
    "        print(f\"\\nRe-running merge using '{id_col}'...\")\n",
    "\n",
    "        traffic_df_linked = pd.merge(\n",
    "            traffic_df.copy(),\n",
    "            coords_to_merge,\n",
    "            on=id_col,\n",
    "            how='left',\n",
    "            suffixes=('_orig', '_clean')\n",
    "        )\n",
    "        print(\"Merge completed.\")\n",
    "\n",
    "        lat_col_merged = lat_col + '_clean'\n",
    "        lon_col_merged = lon_col + '_clean'\n",
    "        print(f\"Clean coordinate columns identified as: '{lat_col_merged}', '{lon_col_merged}'\")\n",
    "\n",
    "        # --- INVESTIGATION of Merge Result (Optional but good practice) ---\n",
    "        print(\"\\n--- Checking State Immediately After Merge ---\")\n",
    "        if lat_col_merged in traffic_df_linked.columns and lon_col_merged in traffic_df_linked.columns:\n",
    "            nan_lat_check = traffic_df_linked[lat_col_merged].isna().sum()\n",
    "            nan_lon_check = traffic_df_linked[lon_col_merged].isna().sum()\n",
    "            print(f\"NaNs detected in '{lat_col_merged}' right after merge: {nan_lat_check} (Expected 0)\") # Expect 0 now\n",
    "            print(f\"NaNs detected in '{lon_col_merged}' right after merge: {nan_lon_check} (Expected 0)\") # Expect 0 now\n",
    "\n",
    "            # Check for 0.0 values specifically\n",
    "            zero_lat_check = (traffic_df_linked[lat_col_merged] == 0.0).sum()\n",
    "            zero_lon_check = (traffic_df_linked[lon_col_merged] == 0.0).sum()\n",
    "            print(f\"Zeroes detected in '{lat_col_merged}' right after merge: {zero_lat_check}\")\n",
    "            print(f\"Zeroes detected in '{lon_col_merged}' right after merge: {zero_lon_check}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Could not find expected merged columns: '{lat_col_merged}', '{lon_col_merged}'\")\n",
    "\n",
    "\n",
    "        # --- Step 17: Filter using notna() AND != 0.0 ---\n",
    "        print(\"\\nAttempting filtering using notna() AND != 0.0 ...\")\n",
    "        if lat_col_merged in traffic_df_linked.columns and lon_col_merged in traffic_df_linked.columns:\n",
    "            # *** THIS IS THE MODIFIED MASK ***\n",
    "            mask = (traffic_df_linked[lat_col_merged].notna() &\n",
    "                    traffic_df_linked[lon_col_merged].notna() &\n",
    "                    (traffic_df_linked[lat_col_merged] != 0.0) &\n",
    "                    (traffic_df_linked[lon_col_merged] != 0.0))\n",
    "\n",
    "            final_df = traffic_df_linked[mask].copy() # Use the mask\n",
    "            final_df = final_df.reset_index(drop=True)\n",
    "            print(\"Filtering complete.\")\n",
    "\n",
    "            # --- Verification ---\n",
    "            print(\"\\n--- After Filtering ---\")\n",
    "            rows_after = len(final_df)\n",
    "            # *** UPDATE EXPECTED ROW COUNT if necessary - Do you know how many rows site 4266 had? ***\n",
    "            # If site 4266 had 31 days of data, expected rows might be 4192 - 31 = 4161?\n",
    "            # Or maybe the original 1240 expectation was correct and other sites also had 0,0?\n",
    "            print(f\"Rows after: {rows_after}\") # Check this number carefully\n",
    "            nan_lat_after = final_df[lat_col_merged].isna().sum()\n",
    "            nan_lon_after = final_df[lon_col_merged].isna().sum()\n",
    "            zero_lat_after = (final_df[lat_col_merged] == 0.0).sum()\n",
    "            zero_lon_after = (final_df[lon_col_merged] == 0.0).sum()\n",
    "            print(f\"NaNs in '{lat_col_merged}' after: {nan_lat_after} (Expected 0)\")\n",
    "            print(f\"NaNs in '{lon_col_merged}' after: {nan_lon_after} (Expected 0)\")\n",
    "            print(f\"Zeroes in '{lat_col_merged}' after: {zero_lat_after} (Expected 0)\")\n",
    "            print(f\"Zeroes in '{lon_col_merged}' after: {zero_lon_after} (Expected 0)\")\n",
    "\n",
    "            num_unique_sites = final_df['SCATS Number'].nunique()\n",
    "            # If site 4266 was the only one with 0,0, expected sites might be 39?\n",
    "            print(f\"Unique SCATS sites remaining: {num_unique_sites}\") # Check this number carefully\n",
    "\n",
    "            # Check if the filtering worked as expected before renaming\n",
    "            # Adjust the expected rows/sites check if needed based on your data understanding\n",
    "            EXPECTED_ROWS_AFTER_FILTER = 1240 # Or 4161, or another number? Let's try 1240 first.\n",
    "            EXPECTED_SITES_AFTER_FILTER = 40 # Or 39? Let's try 40 first.\n",
    "\n",
    "            if rows_after == EXPECTED_ROWS_AFTER_FILTER and num_unique_sites == EXPECTED_SITES_AFTER_FILTER:\n",
    "                print(f\"\\nFiltering produced the expected result ({EXPECTED_ROWS_AFTER_FILTER} rows, {EXPECTED_SITES_AFTER_FILTER} sites).\")\n",
    "                final_lat_col_simple = 'Latitude_clean'\n",
    "                final_lon_col_simple = 'Longitude_clean'\n",
    "                rename_dict = {}\n",
    "                if lat_col_merged in final_df.columns and lat_col_merged != final_lat_col_simple:\n",
    "                    rename_dict[lat_col_merged] = final_lat_col_simple\n",
    "                if lon_col_merged in final_df.columns and lon_col_merged != final_lon_col_simple:\n",
    "                    rename_dict[lon_col_merged] = final_lon_col_simple\n",
    "                if rename_dict:\n",
    "                    final_df = final_df.rename(columns=rename_dict)\n",
    "                    print(f\"Renamed coordinate columns in final_df to '{final_lat_col_simple}', '{final_lon_col_simple}'\")\n",
    "                    lat_col_merged = final_lat_col_simple # Update names for head display\n",
    "                    lon_col_merged = final_lon_col_simple\n",
    "\n",
    "                print(\"\\n--- final_df Head (showing key cols) ---\")\n",
    "                # Add V00, V01 etc to see traffic data too\n",
    "                cols_to_show = ['SCATS Number', 'Date', 'Location', lat_col_merged, lon_col_merged]\n",
    "                # Dynamically add V columns if they exist\n",
    "                v_cols = [col for col in final_df.columns if col.startswith('V')][:5] # Show first 5 V columns\n",
    "                cols_to_show.extend(v_cols)\n",
    "                print(final_df[cols_to_show].head())\n",
    "            else:\n",
    "                print(f\"\\nFiltering did NOT produce the expected result.\")\n",
    "                print(f\"  Expected: {EXPECTED_ROWS_AFTER_FILTER} rows, {EXPECTED_SITES_AFTER_FILTER} sites.\")\n",
    "                print(f\"  Actual:   {rows_after} rows, {num_unique_sites} sites.\")\n",
    "                print(\"  Skipping rename and final head display. Please investigate.\")\n",
    "                # Optionally print head even if incorrect to help debug\n",
    "                print(\"\\n--- final_df Head (current state, showing key cols) ---\")\n",
    "                cols_to_show = ['SCATS Number', 'Date', 'Location', lat_col_merged, lon_col_merged]\n",
    "                v_cols = [col for col in final_df.columns if col.startswith('V')][:5]\n",
    "                cols_to_show.extend(v_cols)\n",
    "                if lat_col_merged in final_df.columns and lon_col_merged in final_df.columns:\n",
    "                     print(final_df[cols_to_show].head())\n",
    "                else:\n",
    "                     print(\"Merged coordinate columns missing from final_df.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping filtering because coordinate columns were not found after merge.\")\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: A key column name is missing or misspelled: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Required DataFrames ('traffic_df' or 'site_coords_unique') not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dfff1-200d-4a6f-b816-0ca8023cfe90",
   "metadata": {},
   "source": [
    "- Step 17: Handle Missing Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1be767d-19fe-421e-88b1-f13a0c56e228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting site_coords_unique DataFrame ---\n",
      "Checking DataFrame shape: (40, 3)\n",
      "Columns: ['SCATS Number', 'NB_LATITUDE', 'NB_LONGITUDE']\n",
      "Data types:\n",
      "SCATS Number      int64\n",
      "NB_LATITUDE     float64\n",
      "NB_LONGITUDE    float64\n",
      "dtype: object\n",
      "\n",
      "Data for SCATS ID 4266 in site_coords_unique:\n",
      "    SCATS Number  NB_LATITUDE  NB_LONGITUDE\n",
      "31          4266    -37.82529     145.04387\n",
      "\n",
      "Is Latitude == 0.0 for site 4266? False\n",
      "Is Longitude == 0.0 for site 4266? False\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Inspecting site_coords_unique DataFrame ---\")\n",
    "\n",
    "# Ensure the DataFrame exists\n",
    "if 'site_coords_unique' in locals() and site_coords_unique is not None:\n",
    "    target_site = 4266\n",
    "    lat_col = 'NB_LATITUDE' # Column name in site_coords_unique\n",
    "    lon_col = 'NB_LONGITUDE'# Column name in site_coords_unique\n",
    "\n",
    "    print(f\"Checking DataFrame shape: {site_coords_unique.shape}\")\n",
    "    print(f\"Columns: {site_coords_unique.columns.tolist()}\")\n",
    "    print(f\"Data types:\\n{site_coords_unique.dtypes}\\n\")\n",
    "\n",
    "    # Check if the target site exists\n",
    "    site_4266_data = site_coords_unique[site_coords_unique['SCATS Number'] == target_site]\n",
    "\n",
    "    if not site_4266_data.empty:\n",
    "        print(f\"Data for SCATS ID {target_site} in site_coords_unique:\")\n",
    "        print(site_4266_data)\n",
    "\n",
    "        # Explicitly check for 0.0 values for this site\n",
    "        is_lat_zero = (site_4266_data[lat_col] == 0.0).any()\n",
    "        is_lon_zero = (site_4266_data[lon_col] == 0.0).any()\n",
    "        print(f\"\\nIs Latitude == 0.0 for site {target_site}? {is_lat_zero}\")\n",
    "        print(f\"Is Longitude == 0.0 for site {target_site}? {is_lon_zero}\")\n",
    "    else:\n",
    "        print(f\"SCATS ID {target_site} NOT FOUND in site_coords_unique.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: DataFrame 'site_coords_unique' not found. Please run the cell that creates it first.\")\n",
    "\n",
    "# Optional: If you want to double-check the raw file loading step:\n",
    "# raw_coord_file = os.path.join(RAW_DATA_DIR, 'Traffic_Count_Locations_with_LONG_LAT.csv')\n",
    "# try:\n",
    "#     raw_coords_df = pd.read_csv(raw_coord_file)\n",
    "#     print(\"\\n--- Checking Raw Coordinate File ---\")\n",
    "#     raw_site_4266 = raw_coords_df[raw_coords_df['SCATS Number'] == target_site]\n",
    "#     if not raw_site_4266.empty:\n",
    "#         print(f\"Data for SCATS ID {target_site} in RAW file ({raw_coord_file}):\")\n",
    "#         # Display relevant columns, adjust names if different in raw file\n",
    "#         print(raw_site_4266[['SCATS Number', 'NB_LATITUDE', 'NB_LONGITUDE']].head())\n",
    "#     else:\n",
    "#         print(f\"SCATS ID {target_site} NOT FOUND in raw file {raw_coord_file}.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"\\nError: Raw coordinate file not found at {raw_coord_file}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError reading raw coordinate file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba18b5-762a-4399-b1dc-f52ec7675972",
   "metadata": {},
   "source": [
    "- final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba707a86-312f-4244-8b88-c70309053256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reshaping final_df from Wide to Long Format ---\n",
      "Melt operation complete.\n",
      "Creating Timestamp column...\n",
      "Renamed coordinate columns to: ['Latitude', 'Longitude']\n",
      "Timestamp creation complete.\n",
      "\n",
      "--- df_long Head ---\n",
      "   SCATS Number                         Location  Latitude  Longitude  \\\n",
      "0           970  WARRIGAL_RD N of HIGH STREET_RD -37.86703  145.09159   \n",
      "1           970  WARRIGAL_RD N of HIGH STREET_RD -37.86703  145.09159   \n",
      "2           970  WARRIGAL_RD N of HIGH STREET_RD -37.86703  145.09159   \n",
      "3           970  WARRIGAL_RD N of HIGH STREET_RD -37.86703  145.09159   \n",
      "4           970  WARRIGAL_RD N of HIGH STREET_RD -37.86703  145.09159   \n",
      "\n",
      "   Timestamp  Volume  \n",
      "0 2006-10-01      86  \n",
      "1 2006-10-02      32  \n",
      "2 2006-10-03      26  \n",
      "3 2006-10-04      32  \n",
      "4 2006-10-05      40  \n",
      "\n",
      "--- df_long Tail ---\n",
      "        SCATS Number                     Location  Latitude  Longitude  \\\n",
      "402427          4821  VICTORIA_ST W OF BURNLEY_ST -37.81285  145.00849   \n",
      "402428          4821  VICTORIA_ST W OF BURNLEY_ST -37.81285  145.00849   \n",
      "402429          4821  VICTORIA_ST W OF BURNLEY_ST -37.81285  145.00849   \n",
      "402430          4821  VICTORIA_ST W OF BURNLEY_ST -37.81285  145.00849   \n",
      "402431          4821  VICTORIA_ST W OF BURNLEY_ST -37.81285  145.00849   \n",
      "\n",
      "                 Timestamp  Volume  \n",
      "402427 2006-10-27 23:45:00      88  \n",
      "402428 2006-10-28 23:45:00     107  \n",
      "402429 2006-10-29 23:45:00      45  \n",
      "402430 2006-10-30 23:45:00      62  \n",
      "402431 2006-10-31 23:45:00      54  \n",
      "\n",
      "--- df_long Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 402432 entries, 0 to 402431\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   SCATS Number  402432 non-null  int64         \n",
      " 1   Location      402432 non-null  object        \n",
      " 2   Latitude      402432 non-null  float64       \n",
      " 3   Longitude     402432 non-null  float64       \n",
      " 4   Timestamp     402432 non-null  datetime64[ns]\n",
      " 5   Volume        402432 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(1)\n",
      "memory usage: 18.4+ MB\n",
      "\n",
      "--- df_long Description (Volume) ---\n",
      "count    402432.000000\n",
      "mean        103.980794\n",
      "std          86.716694\n",
      "min           0.000000\n",
      "25%          26.000000\n",
      "50%          93.000000\n",
      "75%         156.000000\n",
      "max         695.000000\n",
      "Name: Volume, dtype: float64\n",
      "\n",
      "DataFrame reshaped from wide to long format.\n",
      "Original shape (final_df): (4192, 108)\n",
      "New shape (df_long): (402432, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # For timedelta calculation\n",
    "\n",
    "print(\"--- Reshaping final_df from Wide to Long Format ---\")\n",
    "\n",
    "# Ensure final_df exists and has data\n",
    "if 'final_df' in locals() and not final_df.empty:\n",
    "\n",
    "    # Identify the coordinate columns actually present in final_df\n",
    "    # After cell 27 failed the check, renaming didn't happen,\n",
    "    # so the columns are likely still NB_LATITUDE_clean / NB_LONGITUDE_clean\n",
    "    lat_col_final = 'NB_LATITUDE_clean' if 'NB_LATITUDE_clean' in final_df.columns else 'Latitude_clean'\n",
    "    lon_col_final = 'NB_LONGITUDE_clean' if 'NB_LONGITUDE_clean' in final_df.columns else 'Longitude_clean'\n",
    "\n",
    "    # Define ID columns (columns to keep as identifiers)\n",
    "    # Make sure these column names exactly match those in your final_df\n",
    "    id_cols = ['SCATS Number', 'Location', lat_col_final, lon_col_final, 'Date']\n",
    "\n",
    "    # Define Value columns (the columns representing the 15-min intervals)\n",
    "    value_cols = [f'V{i:02d}' for i in range(96)] # V00, V01, ..., V95\n",
    "\n",
    "    # Check if all expected columns exist before melting\n",
    "    missing_id_cols = [col for col in id_cols if col not in final_df.columns]\n",
    "    # Check only a subset of value cols for existence to avoid long error messages\n",
    "    missing_value_cols_sample = [col for col in value_cols[:5] + value_cols[-5:] if col not in final_df.columns]\n",
    "\n",
    "    if missing_id_cols:\n",
    "        print(f\"ERROR: Missing required ID columns in final_df: {missing_id_cols}\")\n",
    "    elif missing_value_cols_sample:\n",
    "        print(f\"ERROR: Missing some Value columns (V00-V95) in final_df. Sample missing: {missing_value_cols_sample}\")\n",
    "    else:\n",
    "        # Melt the DataFrame\n",
    "        df_long = pd.melt(final_df,\n",
    "                          id_vars=id_cols,\n",
    "                          value_vars=value_cols,\n",
    "                          var_name='TimeIntervalCode', # Column name for V00, V01 etc.\n",
    "                          value_name='Volume')         # Column name for the traffic volume\n",
    "        print(\"Melt operation complete.\")\n",
    "\n",
    "        # --- Create a proper Timestamp ---\n",
    "        print(\"Creating Timestamp column...\")\n",
    "        try:\n",
    "            # 1. Convert 'Date' column to datetime objects (important!)\n",
    "            # It might already contain time info (00:15:00) from initial loading,\n",
    "            # let's ensure it's just the date part first for clean calculation.\n",
    "            # If it's already datetime, date() will extract the date part.\n",
    "            if pd.api.types.is_datetime64_any_dtype(df_long['Date']):\n",
    "                 df_long['DateOnly'] = pd.to_datetime(df_long['Date'].dt.date)\n",
    "            else:\n",
    "                 # If it's a string, parse it then get the date\n",
    "                 df_long['DateOnly'] = pd.to_datetime(pd.to_datetime(df_long['Date'], errors='coerce').dt.date)\n",
    "\n",
    "\n",
    "            # 2. Extract the interval number from 'TimeIntervalCode' (e.g., V00 -> 0, V01 -> 1)\n",
    "            df_long['IntervalNumber'] = df_long['TimeIntervalCode'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "            # 3. Calculate the time offset in minutes\n",
    "            df_long['TimeOffset'] = pd.to_timedelta(df_long['IntervalNumber'] * 15, unit='m')\n",
    "\n",
    "            # 4. Create the final Timestamp column by adding offset to the DATE ONLY\n",
    "            df_long['Timestamp'] = df_long['DateOnly'] + df_long['TimeOffset']\n",
    "\n",
    "            # 5. Select and reorder columns, drop intermediate ones\n",
    "            final_cols_order = ['SCATS Number', 'Location', lat_col_final, lon_col_final, 'Timestamp', 'Volume']\n",
    "            df_long = df_long[final_cols_order]\n",
    "\n",
    "            # Rename coordinate columns for simplicity if they have '_clean' suffix\n",
    "            rename_dict = {}\n",
    "            if lat_col_final == 'NB_LATITUDE_clean': rename_dict[lat_col_final] = 'Latitude'\n",
    "            if lon_col_final == 'NB_LONGITUDE_clean': rename_dict[lon_col_final] = 'Longitude'\n",
    "            if rename_dict:\n",
    "                 df_long = df_long.rename(columns=rename_dict)\n",
    "                 print(f\"Renamed coordinate columns to: {list(rename_dict.values())}\")\n",
    "\n",
    "\n",
    "            # --- Validation and Output ---\n",
    "            print(\"Timestamp creation complete.\")\n",
    "\n",
    "            # Check for any NaT values created during timestamp conversion\n",
    "            nat_timestamps = df_long['Timestamp'].isna().sum()\n",
    "            if nat_timestamps > 0:\n",
    "                print(f\"Warning: Created {nat_timestamps} rows with invalid Timestamps (NaT).\")\n",
    "                # df_long.dropna(subset=['Timestamp'], inplace=True) # Optional: Drop NaT rows\n",
    "\n",
    "            # Check Volume data type and NaNs\n",
    "            df_long['Volume'] = pd.to_numeric(df_long['Volume'], errors='coerce')\n",
    "            invalid_volumes = df_long['Volume'].isna().sum()\n",
    "            if invalid_volumes > 0:\n",
    "                 print(f\"Warning: Found {invalid_volumes} rows with non-numeric or NaN 'Volume'.\")\n",
    "                 # df_long['Volume'].fillna(0, inplace=True) # Optional: Fill NaN volumes\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n--- df_long Head ---\")\n",
    "            print(df_long.head())\n",
    "            print(\"\\n--- df_long Tail ---\")\n",
    "            print(df_long.tail())\n",
    "            print(\"\\n--- df_long Info ---\")\n",
    "            df_long.info()\n",
    "            print(\"\\n--- df_long Description (Volume) ---\")\n",
    "            print(df_long['Volume'].describe())\n",
    "\n",
    "            print(f\"\\nDataFrame reshaped from wide to long format.\")\n",
    "            print(f\"Original shape (final_df): {final_df.shape}\") # e.g., (4192, ~106)\n",
    "            print(f\"New shape (df_long): {df_long.shape}\") # e.g., (4192*96, 6)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during timestamp creation or processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"Error: DataFrame 'final_df' not found or is empty. Please ensure cell 27 ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1424133-fc88-4577-9ec2-80de6294748c",
   "metadata": {},
   "source": [
    "- chronological data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31fcac08-3feb-4af8-a271-27f00118aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Splitting Data Chronologically ---\n",
      "df_train shape: (278016, 6)\n",
      "  Train Date Range: 2006-10-01 00:00:00 to 2006-10-21 23:45:00\n",
      "df_val shape: (61536, 6)\n",
      "  Validation Date Range: 2006-10-22 00:00:00 to 2006-10-26 23:45:00\n",
      "df_test shape: (62880, 6)\n",
      "  Test Date Range: 2006-10-27 00:00:00 to 2006-10-31 23:45:00\n",
      "\n",
      "Total rows in splits: 402432 (Original df_long: 402432)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"--- Splitting Data Chronologically ---\")\n",
    "\n",
    "# Ensure df_long exists\n",
    "if 'df_long' in locals() and not df_long.empty:\n",
    "\n",
    "    # Make sure Timestamp is the index or sorted for easy slicing\n",
    "    # Sorting is safer if the melt didn't perfectly order everything by time\n",
    "    df_long = df_long.sort_values(by='Timestamp')\n",
    "\n",
    "    # Define cutoff dates (inclusive for start, exclusive for end)\n",
    "    train_end_date = '2006-10-22' # Train includes up to 2006-10-21 23:59:59\n",
    "    val_end_date = '2006-10-27'   # Validation includes up to 2006-10-26 23:59:59\n",
    "                                # Test starts from 2006-10-27 00:00:00\n",
    "\n",
    "    # Perform the split\n",
    "    df_train = df_long[df_long['Timestamp'] < train_end_date]\n",
    "    df_val = df_long[(df_long['Timestamp'] >= train_end_date) & (df_long['Timestamp'] < val_end_date)]\n",
    "    df_test = df_long[df_long['Timestamp'] >= val_end_date]\n",
    "\n",
    "    # Verify the shapes and date ranges\n",
    "    print(f\"df_train shape: {df_train.shape}\")\n",
    "    if not df_train.empty:\n",
    "        print(f\"  Train Date Range: {df_train['Timestamp'].min()} to {df_train['Timestamp'].max()}\")\n",
    "\n",
    "    print(f\"df_val shape: {df_val.shape}\")\n",
    "    if not df_val.empty:\n",
    "        print(f\"  Validation Date Range: {df_val['Timestamp'].min()} to {df_val['Timestamp'].max()}\")\n",
    "\n",
    "    print(f\"df_test shape: {df_test.shape}\")\n",
    "    if not df_test.empty:\n",
    "        print(f\"  Test Date Range: {df_test['Timestamp'].min()} to {df_test['Timestamp'].max()}\")\n",
    "\n",
    "    # Check if the total rows match\n",
    "    total_rows = len(df_train) + len(df_val) + len(df_test)\n",
    "    print(f\"\\nTotal rows in splits: {total_rows} (Original df_long: {len(df_long)})\")\n",
    "    if total_rows != len(df_long):\n",
    "        print(\"Warning: Row count mismatch after splitting!\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: DataFrame 'df_long' not found or is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23100-30bf-4a51-8720-bf6ba8c73359",
   "metadata": {},
   "source": [
    "- scale and volume feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d7e5d7a-edde-42d6-8828-10ca382a6d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scaling Volume Feature ---\n",
      "Fitting scaler on training data (shape: (278016, 1))...\n",
      "Scaler fitted.\n",
      "Transforming train, validation, and test data...\n",
      "Transformation complete.\n",
      "\n",
      "--- Scaled Volume Stats ---\n",
      "Train Volume Min: 0.0000, Max: 1.0000\n",
      "Val Volume Min:   0.0000, Max: 1.0928\n",
      "Test Volume Min:  0.0000, Max: 0.9686\n",
      "\n",
      "--- df_train_scaled Head ---\n",
      "      SCATS Number                         Location   Latitude   Longitude  \\\n",
      "0              970  WARRIGAL_RD N of HIGH STREET_RD -37.867030  145.091590   \n",
      "3092          4263         POWER_ST N of BURWOOD_RD -37.822846  145.025129   \n",
      "3066          4262       BRIDGE_RD SW of BURWOOD_RD -37.821550  145.015030   \n",
      "3035          4063     WHITEHORSE_RD W OF BALWYN_RD -37.814040  145.080100   \n",
      "3004          4063     BALWYN_RD S OF WHITEHORSE_RD -37.814040  145.080100   \n",
      "\n",
      "      Timestamp    Volume  \n",
      "0    2006-10-01  0.135220  \n",
      "3092 2006-10-01  0.062893  \n",
      "3066 2006-10-01  0.190252  \n",
      "3035 2006-10-01  0.066038  \n",
      "3004 2006-10-01  0.048742  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np # For reshaping\n",
    "\n",
    "print(\"--- Scaling Volume Feature ---\")\n",
    "\n",
    "# Ensure the split DataFrames exist\n",
    "if ('df_train' in locals() and 'df_val' in locals() and 'df_test' in locals()):\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    # feature_range=(0, 1) is the default\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # --- Fit Scaler on Training Data ---\n",
    "    # Scaler expects a 2D array, so reshape the Volume column\n",
    "    # .values converts the pandas Series to a NumPy array\n",
    "    # .reshape(-1, 1) converts it from (n_samples,) to (n_samples, 1)\n",
    "    train_volume = df_train['Volume'].values.reshape(-1, 1)\n",
    "\n",
    "    print(f\"Fitting scaler on training data (shape: {train_volume.shape})...\")\n",
    "    scaler.fit(train_volume)\n",
    "    print(\"Scaler fitted.\")\n",
    "\n",
    "    # --- Transform Train, Validation, and Test Data ---\n",
    "    # Apply the *fitted* scaler to all three sets\n",
    "    # Important: Use .copy() to avoid SettingWithCopyWarning when assigning back\n",
    "    print(\"Transforming train, validation, and test data...\")\n",
    "\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_val_scaled = df_val.copy()\n",
    "    df_test_scaled = df_test.copy()\n",
    "\n",
    "    # Transform and overwrite the 'Volume' column\n",
    "    # (Alternatively, create a new 'Volume_scaled' column)\n",
    "    df_train_scaled['Volume'] = scaler.transform(df_train['Volume'].values.reshape(-1, 1))\n",
    "    df_val_scaled['Volume'] = scaler.transform(df_val['Volume'].values.reshape(-1, 1))\n",
    "    df_test_scaled['Volume'] = scaler.transform(df_test['Volume'].values.reshape(-1, 1))\n",
    "\n",
    "    print(\"Transformation complete.\")\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(\"\\n--- Scaled Volume Stats ---\")\n",
    "    print(f\"Train Volume Min: {df_train_scaled['Volume'].min():.4f}, Max: {df_train_scaled['Volume'].max():.4f}\")\n",
    "    print(f\"Val Volume Min:   {df_val_scaled['Volume'].min():.4f}, Max: {df_val_scaled['Volume'].max():.4f}\")\n",
    "    print(f\"Test Volume Min:  {df_test_scaled['Volume'].min():.4f}, Max: {df_test_scaled['Volume'].max():.4f}\")\n",
    "\n",
    "    # Display head of the scaled training data to see the result\n",
    "    print(\"\\n--- df_train_scaled Head ---\")\n",
    "    print(df_train_scaled.head())\n",
    "\n",
    "    # Keep the scaled dataframes for the next step\n",
    "    # Optionally overwrite original dfs: df_train, df_val, df_test = df_train_scaled, df_val_scaled, df_test_scaled\n",
    "    # For clarity let's keep them separate for now\n",
    "\n",
    "else:\n",
    "    print(\"Error: Split DataFrames ('df_train', 'df_val', 'df_test') not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e189694-a77a-4bdf-ae66-b1693cdf81d9",
   "metadata": {},
   "source": [
    "- generate input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9511bd6b-fdd6-4cc8-bbf2-30a14098c2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Sequences for LSTM/GRU ---\n",
      "\n",
      "Generating training sequences...\n",
      "Processing 40 sites...\n",
      "  Processing site 10/40 (ID: 3120)\n",
      "  Processing site 20/40 (ID: 4030)\n",
      "  Processing site 30/40 (ID: 4263)\n",
      "  Processing site 40/40 (ID: 4821)\n",
      "Sequence generation complete for all sites.\n",
      "X_train shape: (277856, 4, 1)\n",
      "y_train shape: (277856,)\n",
      "\n",
      "Generating validation sequences...\n",
      "Processing 40 sites...\n",
      "  Processing site 10/40 (ID: 3120)\n",
      "  Processing site 20/40 (ID: 4030)\n",
      "  Processing site 30/40 (ID: 4263)\n",
      "  Processing site 40/40 (ID: 4821)\n",
      "Sequence generation complete for all sites.\n",
      "X_val shape: (61376, 4, 1)\n",
      "y_val shape: (61376,)\n",
      "\n",
      "Generating test sequences...\n",
      "Processing 40 sites...\n",
      "  Processing site 10/40 (ID: 3120)\n",
      "  Processing site 20/40 (ID: 4030)\n",
      "  Processing site 30/40 (ID: 4263)\n",
      "  Processing site 40/40 (ID: 4821)\n",
      "Sequence generation complete for all sites.\n",
      "X_test shape: (62720, 4, 1)\n",
      "y_test shape: (62720,)\n",
      "\n",
      "Sequence data ready for model training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Generating Sequences for LSTM/GRU ---\")\n",
    "\n",
    "def create_sequences(input_data, n_input, n_output, feature_col='Volume'):\n",
    "    \"\"\"\n",
    "    Generates input sequences (X) and output targets (y)\n",
    "    for time series forecasting, grouped by SCATS Number.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    # Ensure data is sorted by Timestamp within each group\n",
    "    input_data = input_data.sort_values(by=['SCATS Number', 'Timestamp'])\n",
    "    grouped = input_data.groupby('SCATS Number')\n",
    "\n",
    "    total_sites = len(grouped)\n",
    "    print(f\"Processing {total_sites} sites...\")\n",
    "    site_count = 0\n",
    "\n",
    "    for site_id, group in grouped:\n",
    "        site_count += 1\n",
    "        if site_count % 10 == 0: # Print progress every 10 sites\n",
    "            print(f\"  Processing site {site_count}/{total_sites} (ID: {site_id})\")\n",
    "\n",
    "        # Extract the time series for the relevant feature\n",
    "        # .values converts to NumPy array\n",
    "        time_series = group[feature_col].values\n",
    "\n",
    "        # Iterate through the time series to create sequences\n",
    "        for i in range(len(time_series) - n_input - n_output + 1):\n",
    "            input_seq = time_series[i : i + n_input]\n",
    "            output_val = time_series[i + n_input : i + n_input + n_output]\n",
    "\n",
    "            X.append(input_seq)\n",
    "            y.append(output_val)\n",
    "\n",
    "    print(\"Sequence generation complete for all sites.\")\n",
    "    # Convert lists to NumPy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Reshape X for LSTM/GRU input: (n_samples, n_timesteps, n_features)\n",
    "    # In this case, n_features is 1 because we only use 'Volume'\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    # If n_output is 1, reshape y to be (n_samples,) instead of (n_samples, 1)\n",
    "    if n_output == 1:\n",
    "        y = y.ravel()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# --- Define sequence parameters ---\n",
    "N_INPUT_STEPS = 4  # Use previous 1 hour (4 * 15 mins)\n",
    "N_OUTPUT_STEPS = 1 # Predict next 15 mins\n",
    "\n",
    "# Ensure the scaled DataFrames exist\n",
    "if ('df_train_scaled' in locals() and 'df_val_scaled' in locals() and 'df_test_scaled' in locals()):\n",
    "\n",
    "    print(\"\\nGenerating training sequences...\")\n",
    "    X_train, y_train = create_sequences(df_train_scaled, N_INPUT_STEPS, N_OUTPUT_STEPS)\n",
    "    print(f\"X_train shape: {X_train.shape}\") # Expected: (samples, 4, 1)\n",
    "    print(f\"y_train shape: {y_train.shape}\") # Expected: (samples,)\n",
    "\n",
    "    print(\"\\nGenerating validation sequences...\")\n",
    "    X_val, y_val = create_sequences(df_val_scaled, N_INPUT_STEPS, N_OUTPUT_STEPS)\n",
    "    print(f\"X_val shape: {X_val.shape}\")     # Expected: (samples, 4, 1)\n",
    "    print(f\"y_val shape: {y_val.shape}\")     # Expected: (samples,)\n",
    "\n",
    "    print(\"\\nGenerating test sequences...\")\n",
    "    X_test, y_test = create_sequences(df_test_scaled, N_INPUT_STEPS, N_OUTPUT_STEPS)\n",
    "    print(f\"X_test shape: {X_test.shape}\")   # Expected: (samples, 4, 1)\n",
    "    print(f\"y_test shape: {y_test.shape}\")   # Expected: (samples,)\n",
    "\n",
    "    print(\"\\nSequence data ready for model training.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Scaled DataFrames ('df_train_scaled', etc.) not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c67a3-9410-4aad-9ad1-2cf096062b4c",
   "metadata": {},
   "source": [
    "- Build and Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "415c1b26-afd6-491e-8ced-250df7adff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building and Training LSTM Model ---\n",
      "\n",
      "--- LSTM Model Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/10\n",
      "\u001b[1m 444/4342\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 33ms/step - loss: 0.0129"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m BATCH_SIZE = \u001b[32m64\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m history_lstm = \u001b[43mmodel_lstm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Show progress bar\u001b[39;49;00m\n\u001b[32m     67\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Training Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# --- Plot Training History ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1498\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1509\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1510\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1515\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Academics/introToAI/Assignment2B/assignment2b/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"--- Building and Training LSTM Model ---\")\n",
    "\n",
    "# Ensure sequence data exists\n",
    "if ('X_train' in locals() and 'y_train' in locals() and\n",
    "    'X_val' in locals() and 'y_val' in locals()):\n",
    "\n",
    "    # --- Define Model Architecture ---\n",
    "    N_TIMESTEPS = X_train.shape[1] # Should be 4 (N_INPUT_STEPS)\n",
    "    N_FEATURES = X_train.shape[2] # Should be 1 (Volume only)\n",
    "    LSTM_UNITS = 50 # Number of neurons in the LSTM layer (hyperparameter)\n",
    "\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(N_TIMESTEPS, N_FEATURES)),\n",
    "        LSTM(LSTM_UNITS, activation='relu'), # activation='relu' can sometimes work well\n",
    "        Dense(1) # Output layer for single-step prediction\n",
    "    ])\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    # Use Mean Squared Error for regression loss\n",
    "    # Adam is a standard optimizer\n",
    "    model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    print(\"\\n--- LSTM Model Summary ---\")\n",
    "    model_lstm.summary()\n",
    "\n",
    "    # --- Define Callbacks ---\n",
    "    # EarlyStopping: Stop training if validation loss doesn't improve\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', # Monitor validation loss\n",
    "        patience=5,         # Stop after 5 epochs of no improvement\n",
    "        restore_best_weights=True # Restore weights from the best epoch\n",
    "    )\n",
    "\n",
    "    # ModelCheckpoint: Save the best model found during training\n",
    "    # Create a directory to save models if it doesn't exist\n",
    "    MODEL_SAVE_DIR = 'saved_models'\n",
    "    if not os.path.exists(MODEL_SAVE_DIR):\n",
    "        os.makedirs(MODEL_SAVE_DIR)\n",
    "    checkpoint_filepath = os.path.join(MODEL_SAVE_DIR, 'best_lstm_model.keras')\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True, # Only save when val_loss improves\n",
    "        save_weights_only=False # Save the full model\n",
    "    )\n",
    "\n",
    "    # --- Train Model ---\n",
    "    EPOCHS = 10 \n",
    "    BATCH_SIZE = 64 \n",
    "\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    history_lstm = model_lstm.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1 # Show progress bar\n",
    "    )\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "    # --- Plot Training History ---\n",
    "    print(\"\\n--- Plotting Training History ---\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history_lstm.history['loss'], label='Training Loss')\n",
    "    plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('LSTM Model Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error (Loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nBest LSTM model saved to: {checkpoint_filepath}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Sequence data (X_train, y_train, etc.) not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
